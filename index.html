<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering">
  <meta name="keywords" content="Mind palace exploration, long term eqa, embodied ai, robotics, planning, reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering
  </title>
  

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering</h1>
          <div class="is-size-5 publication-authors">
            <p><strong>Under review.</strong></p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
            </div>

          </div>
        </div>
      </div>
    </div>
    
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="90%">
        <source src="./static/videos/cover.mp4"
                type="video/mp4"
                style="width: 90%; height: auto;">
      </video>
      <h2 class="subtitle has-text-centered">
  
      </h2>
    </div>

   

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        
        <div class="content has-text-justified">
          <p>
            As robots become increasingly capable of operating over extended periods—spanning days, weeks, and even months—they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively.  
            This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. 
            Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. 
            Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. 
          </p>
          <p>  
            To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. 
            Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. 
            To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determine when the agent has gathered sufficient information. 
            We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. 
            Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.
          </p>

        </div>


      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Long-term Active Embodied Question Answering</h2>

        <div class="column is-centered has-text-centered">
          <img src="./static/images/task.png"
               class="interpolation-image"
               alt="Problem setup."
               style="width: 90%; height: auto;"/>
        </div>
        <div class="content has-text-justified">
          <p>
            We introduce a new task called Long-term Active Embodied Question Answering (LA-EQA), where robots must both recall past experiences and actively explore their surroundings to answer complex questions. 
            Compared to existing active EQA and episodic EQA tasks, LA-EQA unifies these settings and extends the problem to require reasoning over multiple past episodic memories accumulated over extended periods. 
            To the best of our knowledge, this problem is largely unexplored, and no benchmark currently exists to evaluate it.
          </p>

        </div>
        
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Mind Palace Exploration</h2>

                <div class="content has-text-justified">
          <p>Humans use the mind palace technique to remember complex information by organizing it into a structured spatial memory, which enables efficient retrieval and traceable recall of relevant memories. We explore how this technique can be applied to long-term memory representation and reasoning in robots.</p>
          

        </div>

        <div class="column is-centered has-text-centered">
          <img src="./static/images/approach.png"
               class="interpolation-image"
               alt="System Architecture."
               style="width: 90%; height: auto;"/>
        </div>
        <div class="content has-text-justified">
          <!-- <p>Humans use the mind palace technique to remember complex information by organizing it into a structured spatial memory, which enables efficient retrieval and traceable recall of relevant memories. We explore how this technique can be applied to long-term memory representation and reasoning in robots.</p> -->
          
          <p>
            Mind Palace Exploration builds a Robotic Mind Palace that unifies past memories and environment representation (1). 
            The Robotic Mind Palace summarizes the robot’s history of observations into multiple world instances of scene graphs. 
            Given a question (2), the agent alternates between reasoning over the question to identify a target object (3), planning a search strategy through memory retrieval and exploration (4), and updating its working memory (5), until it is ready to answer the question (6). 
            Additionally, we introduce early stopping criteria using the notion of Value of Information to avoid retrieving memory that is unlikely to improve the next exploration action.
        </p>

        </div>
        
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Long-term Active EQA Benchmark</h2>

        <div class="column is-centered has-text-centered">
          <img src="./static/images/benchmark.png"
               class="interpolation-image"
               alt="System Architecture."
               style="width: 90%; height: auto;"/>
        </div>
        <div class="content has-text-justified">
          <p>
            We curate the first LA-EQA dataset and benchmark, consisting of 3 simulated and 2 real-world scenes. 
            For each simulation scene, we generate 5--10 scene variations over multiple days, reflecting changes caused by common routines. 
            For real-world scenes, we collected 11 trajectories (30--60 mins) in an industrial site and an office environment over a 6-month period. 
          </p>

          <p>We categorize the questions based on their required temporal reasoning to capture different aspects of long-term scene understanding:
          <ol>
            <li><b>Past questions</b> pertain to a specific event observed in a single past trajectory.</li>
            <li><b>Present questions</b> require only exploration of the current environment.</li>
            <li><b>Multi-past questions</b> involve synthesizing information from multiple past trajectories (e.g., “What do we usually eat for breakfast?”).</li>
            <li><b>Past-present questions</b> require reasoning over both historical memory and the current scene (e.g., “Are we missing anything we usually have for breakfast?”).</li>
            <li><b>Past-present-future questions</b> involve predicting future outcomes based on both past and present observations (e.g., “When do you think we will run out of apples for breakfast?”).</li>
          </ol>
          </p>


          <!-- <p>
          We categorize the questions based on their required temporal reasoning to capture different aspects of long-term scene understanding:
          1) <b>Past questions</b> pertain to a specific event observed in a single past trajectory.  
          2) <b>Present questions</b> require only exploration of the current environment.  
          3) <b>Multi-past questions</b> involve synthesizing information from multiple past trajectories (e.g., “What do we usually eat for breakfast?”).  
          4) <b>Past-present questions</b> require reasoning over both historical memory and the current scene (e.g., “Are we missing anything we usually have for breakfast?”).  
          5) <b>Past-present-future questions</b> involve predicting future outcomes based on both past and present observations (e.g., “When do you think we will run out of apples for breakfast?”).
          </p> -->
          <p>
          We curated 150 questions, which uniformly cover the question types. The questions were generated by seven people to ensure the diversity of the questions.
          <p>
            


        </div>
        
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website source borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
